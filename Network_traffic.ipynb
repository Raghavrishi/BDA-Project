{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEBxdFWaVLSH"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYJoTYj6VPj9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import reduce\n",
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "spark = SparkSession.builder.master('local[*]').appName('TrafficAnalysisUsingPySpark').getOrCreate()\n",
        "print(spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e_S6g0m4tip"
      },
      "source": [
        "#Data Insert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CIhaWACViQb"
      },
      "outputs": [],
      "source": [
        "DATA = \"/content/drive/MyDrive/BDA project/Dataset-Unicauca-Version2-87Atts.csv\"\n",
        "\n",
        "df = spark.read.options(header='True',inferSchema='True').csv(path=DATA)\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QxqC7ts4pch"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPauJcTEbW0d"
      },
      "outputs": [],
      "source": [
        "current_columns = df.columns\n",
        "\n",
        "new_columns = list(map(lambda item : item.replace(\" \",\"_\").replace(\".\",\"_\").upper().strip(),current_columns))\n",
        "final_df = reduce(lambda data, idx: data.withColumnRenamed(current_columns[idx], new_columns[idx]), range(len(current_columns)), df)\n",
        "final_df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wROb33I0gsDt"
      },
      "outputs": [],
      "source": [
        "final_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBk3gvJ0hV7v"
      },
      "outputs": [],
      "source": [
        "final_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEAwwTImheJ9"
      },
      "outputs": [],
      "source": [
        "sub_df = final_df.select('FLOW_ID',\n",
        " 'SOURCE_IP',\n",
        " 'SOURCE_PORT',\n",
        " 'DESTINATION_IP',\n",
        " 'DESTINATION_PORT',\n",
        " 'PROTOCOL',\n",
        " 'TIMESTAMP',\n",
        " 'FLOW_DURATION',\n",
        " 'TOTAL_FWD_PACKETS',\n",
        " 'TOTAL_BACKWARD_PACKETS',\n",
        " 'TOTAL_LENGTH_OF_FWD_PACKETS',\n",
        " 'TOTAL_LENGTH_OF_BWD_PACKETS',\n",
        " 'FLOW_BYTES_S',\n",
        " 'FLOW_PACKETS_S',\n",
        " 'AVERAGE_PACKET_SIZE',\n",
        " 'LABEL',\n",
        " 'PROTOCOLNAME')\n",
        "sub_df.show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VdxgmURhjSP"
      },
      "outputs": [],
      "source": [
        "sub_df.groupBy(\"PROTOCOLNAME\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg2Sc5wbhmXa"
      },
      "outputs": [],
      "source": [
        "sub_df.select('FLOW_DURATION').describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nRlVACKhqTn"
      },
      "outputs": [],
      "source": [
        "sub_df.select([count(when(isnan(c), c)).alias(c) for c in sub_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqy8DBfDhvf-"
      },
      "outputs": [],
      "source": [
        "sub_df.select('PROTOCOLNAME').distinct().collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBMy-kFdh3_3"
      },
      "outputs": [],
      "source": [
        "socmed = ['TWITTER','INSTAGRAM','FACEBOOK']\n",
        "\n",
        "records= sub_df.filter(sub_df.PROTOCOLNAME.isin(socmed))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcNFabfLj2mP"
      },
      "outputs": [],
      "source": [
        "records_df = records.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PbGbwcFj_yW"
      },
      "outputs": [],
      "source": [
        "records_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM8k7EghkLvq"
      },
      "outputs": [],
      "source": [
        "records_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "records_df[\"PROTOCOLNAME\"].value_counts().plot.bar()"
      ],
      "metadata": {
        "id": "9KizDy6zBTWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.countplot(x = 'PROTOCOLNAME', data = records_df)"
      ],
      "metadata": {
        "id": "FlDGVFajBRLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rOWNBJT4jkI"
      },
      "source": [
        "#ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46MMqbvaiBM0"
      },
      "source": [
        "Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2PqXftxiASg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGohbpzzh4rE"
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder().fit(records_df['PROTOCOLNAME'])\n",
        "records_df['PROTOCOLNAME'] = encoder.fit_transform(records_df['PROTOCOLNAME'])\n",
        "records_df['PROTOCOLNAME']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF8txhIOh-zl"
      },
      "outputs": [],
      "source": [
        "\n",
        "X = records_df.drop(columns = ['FLOW_ID',\n",
        " 'SOURCE_IP',\n",
        " 'SOURCE_PORT',\n",
        " 'DESTINATION_IP',\n",
        " 'DESTINATION_PORT',\n",
        " 'PROTOCOL',\n",
        " 'TIMESTAMP',\n",
        " 'LABEL',\n",
        " 'PROTOCOLNAME'])\n",
        "Y = records_df['PROTOCOLNAME']\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6Z5jEGLkSjD"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciVhwMxakXVg"
      },
      "outputs": [],
      "source": [
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_k1deDTkYhF"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ0Z8dnOkbHa"
      },
      "outputs": [],
      "source": [
        "accuracy_score(pred,Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMc7GDQN376G"
      },
      "source": [
        "Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBabkksdTelb"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model1= RandomForestClassifier(n_estimators= 10, criterion=\"entropy\")\n",
        "model1.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU7fNPiNWz4z"
      },
      "outputs": [],
      "source": [
        "y_pred= model1.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3loKrjrW747"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_pred,Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "_TKY-iwLQGuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1= model1.predict(X_test)"
      ],
      "metadata": {
        "id": "a1zO77wQQHCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_pred1,Y_test)"
      ],
      "metadata": {
        "id": "J8RAuw_1QHKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOg1XgHW4QWL"
      },
      "source": [
        "# Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7tTUY9w4B_5"
      },
      "source": [
        "Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QI9TC_7YIKt"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer,VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier,NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm1ZB6JJdzD6"
      },
      "outputs": [],
      "source": [
        "inputColumns = ['FLOW_DURATION',\t'TOTAL_FWD_PACKETS',\t'TOTAL_BACKWARD_PACKETS',\t'TOTAL_LENGTH_OF_FWD_PACKETS',\t'TOTAL_LENGTH_OF_BWD_PACKETS',\t'FLOW_BYTES_S','FLOW_PACKETS_S',\t'AVERAGE_PACKET_SIZE']\n",
        "outputColumn = \"PROTOCOL_NAME\"\n",
        "indexer = StringIndexer(inputCol=\"PROTOCOLNAME\", outputCol=\"PROTOCOL_NAME\")\n",
        "in_df = indexer.fit(records).transform(records)\n",
        "in_df=in_df.drop(\"PROTOCOLNAME\").withColumnRenamed(\"PROTOCOL_NAME\",\"PROTOCOLNAME\")\n",
        "vector_assembler = VectorAssembler(inputCols=inputColumns, outputCol=outputColumn)\n",
        "rf_model = RandomForestClassifier(labelCol=\"PROTOCOLNAME\", featuresCol=\"PROTOCOL_NAME\")\n",
        "stages = [vector_assembler, rf_model]\n",
        "pipeline1 = Pipeline(stages=stages)\n",
        "in_df=in_df.drop(in_df['LABEL'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRqpZBKud0KC"
      },
      "outputs": [],
      "source": [
        "train, test = in_df.randomSplit([0.7, 0.3], seed = 2018)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOUXC2asrptV"
      },
      "outputs": [],
      "source": [
        "paramGrid_rf = ParamGridBuilder() \\\n",
        "    .addGrid(rf_model.numTrees, [1, 1, 1]) \\\n",
        "    .addGrid(rf_model.maxDepth, [1, 1, 1]) \\\n",
        "    .addGrid(rf_model.featureSubsetStrategy, ['auto','log2']) \\\n",
        "    .build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajkoIvQVrwER"
      },
      "outputs": [],
      "source": [
        "crossval_rf = CrossValidator(estimator=pipeline1, estimatorParamMaps=paramGrid_rf,\n",
        "                             evaluator=MulticlassClassificationEvaluator(\n",
        "                                 labelCol='PROTOCOLNAME', predictionCol='prediction', metricName='accuracy'),\n",
        "                             numFolds=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v7Qsy8AscU6"
      },
      "outputs": [],
      "source": [
        "cvModel_rf =crossval_rf.fit(train)\n",
        "best_model_rf = cvModel_rf.bestModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ7vzl97kX7k"
      },
      "outputs": [],
      "source": [
        "model = pipeline1.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGrEjf1QrgpS"
      },
      "outputs": [],
      "source": [
        "y_pred = model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ockIfe-eyz-z"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from sklearn.metrics import classification_report\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"PROTOCOLNAME\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(y_pred)\n",
        "pd = y_pred.select(\"PROTOCOLNAME\", \"prediction\").toPandas()\n",
        "true_labels = pd[\"PROTOCOLNAME\"].tolist()\n",
        "predicted_labels = pd[\"prediction\"].tolist()\n",
        "report = classification_report(true_labels, predicted_labels)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuUGB20GwMIS"
      },
      "outputs": [],
      "source": [
        "predictions_rf = best_model_rf.transform(test)\n",
        "accuracy_rf = evaluator.evaluate(predictions_rf)\n",
        "print(f\"Test Accuracy for Random Forest: {accuracy_rf:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlP4vpXT4IR9"
      },
      "source": [
        "Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dzOZVcGza1c"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer,VectorAssembler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01Bo0EeB0m-u"
      },
      "outputs": [],
      "source": [
        "inputColumns = ['FLOW_DURATION',\t'TOTAL_FWD_PACKETS',\t'TOTAL_BACKWARD_PACKETS',\t'TOTAL_LENGTH_OF_FWD_PACKETS',\t'TOTAL_LENGTH_OF_BWD_PACKETS',\t'FLOW_BYTES_S','FLOW_PACKETS_S',\t'AVERAGE_PACKET_SIZE']\n",
        "outputColumn = \"PROTOCOL_NAME\"\n",
        "indexer = StringIndexer(inputCol=\"PROTOCOLNAME\", outputCol=\"PROTOCOL_NAME\")\n",
        "in_df = indexer.fit(records).transform(records)\n",
        "in_df=in_df.drop(\"PROTOCOLNAME\").withColumnRenamed(\"PROTOCOL_NAME\",\"PROTOCOLNAME\")\n",
        "vector_assembler = VectorAssembler(inputCols=inputColumns, outputCol=outputColumn)\n",
        "dt_model = DecisionTreeClassifier(labelCol=\"PROTOCOLNAME\", featuresCol=\"PROTOCOL_NAME\")\n",
        "stages = [vector_assembler, dt_model]\n",
        "pipeline2 = Pipeline(stages=stages)\n",
        "in_df=in_df.drop(in_df['LABEL'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yZjdtdg0q37"
      },
      "outputs": [],
      "source": [
        "train1, test1 = in_df.randomSplit([0.7, 0.3], seed = 2018)\n",
        "train1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIY7S6lugN0W"
      },
      "outputs": [],
      "source": [
        "paramGrid_dt = ParamGridBuilder() \\\n",
        "    .addGrid(dt_model.maxDepth, [3, 5, 7]) \\\n",
        "    .addGrid(dt_model.minInstancesPerNode, [1, 3, 5]) \\\n",
        "    .addGrid(dt_model.impurity, ['gini', 'entropy']) \\\n",
        "    .build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC8SJeJZqu88"
      },
      "outputs": [],
      "source": [
        "crossval_dt = CrossValidator(estimator=pipeline2, estimatorParamMaps=paramGrid_dt,\n",
        "                             evaluator=MulticlassClassificationEvaluator(\n",
        "                                 labelCol='PROTOCOLNAME', predictionCol='prediction', metricName='accuracy'),\n",
        "                             numFolds=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgqMi9Nyq2uX"
      },
      "outputs": [],
      "source": [
        "cvModel_dt = crossval_dt.fit(train1)\n",
        "best_model_dt = cvModel_dt.bestModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP6_BM4z027v"
      },
      "outputs": [],
      "source": [
        "model2 = pipeline2.fit(train1)\n",
        "y_pred = model2.transform(test1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iQfh6yB07wH"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from sklearn.metrics import classification_report\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"PROTOCOLNAME\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy2 = evaluator.evaluate(y_pred)\n",
        "pd = y_pred.select(\"PROTOCOLNAME\", \"prediction\").toPandas()\n",
        "true_labels = pd[\"PROTOCOLNAME\"].tolist()\n",
        "predicted_labels = pd[\"prediction\"].tolist()\n",
        "report = classification_report(true_labels, predicted_labels)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-Y7Qy70rT3K"
      },
      "outputs": [],
      "source": [
        "predictions_dt = best_model_dt.transform(test1)\n",
        "accuracy_dt = evaluator.evaluate(predictions_dt)\n",
        "print(f\"Test Accuracy for Decision Tree: {accuracy_dt:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes"
      ],
      "metadata": {
        "id": "aRo7UM0gRQq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputColumns = ['FLOW_DURATION',\t'TOTAL_FWD_PACKETS',\t'TOTAL_BACKWARD_PACKETS',\t'TOTAL_LENGTH_OF_FWD_PACKETS',\t'TOTAL_LENGTH_OF_BWD_PACKETS',\t'FLOW_BYTES_S','FLOW_PACKETS_S',\t'AVERAGE_PACKET_SIZE']\n",
        "outputColumn = \"PROTOCOL_NAME\"\n",
        "indexer = StringIndexer(inputCol=\"PROTOCOLNAME\", outputCol=\"PROTOCOL_NAME\")\n",
        "in_df = indexer.fit(records).transform(records)\n",
        "in_df=in_df.drop(\"PROTOCOLNAME\").withColumnRenamed(\"PROTOCOL_NAME\",\"PROTOCOLNAME\")\n",
        "vector_assembler = VectorAssembler(inputCols=inputColumns, outputCol=outputColumn)\n",
        "nb_model= NaiveBayes(featuresCol='features', labelCol='target')\n",
        "stages = [vector_assembler, dt_model]\n",
        "pipeline3 = Pipeline(stages=stages)\n",
        "in_df=in_df.drop(in_df['LABEL'])"
      ],
      "metadata": {
        "id": "1ssBAqr5RMV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train2, test2 = in_df.randomSplit([0.7, 0.3], seed = 2018)\n",
        "train2"
      ],
      "metadata": {
        "id": "3CN9jqGRRaNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid_nb = ParamGridBuilder() \\\n",
        "    .addGrid(nb_model.smoothing, [0.0, 1.0]) \\\n",
        "    .build()"
      ],
      "metadata": {
        "id": "J2CRmNqJRaX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crossval_nb = CrossValidator(estimator=pipeline3, estimatorParamMaps=paramGrid_nb,\n",
        "                              evaluator=MulticlassClassificationEvaluator(\n",
        "                                  labelCol='label', predictionCol='prediction', metricName='accuracy'),\n",
        "                              numFolds=5)"
      ],
      "metadata": {
        "id": "PrGlKujJRabr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvModel_nb = crossval_nb.fit(train1)\n",
        "best_model_nb = cvModel_nb.bestModel"
      ],
      "metadata": {
        "id": "5kY6uObyRae0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from sklearn.metrics import classification_report\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"PROTOCOLNAME\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy3 = evaluator.evaluate(y_pred)\n",
        "pd = y_pred.select(\"PROTOCOLNAME\", \"prediction\").toPandas()\n",
        "true_labels = pd[\"PROTOCOLNAME\"].tolist()\n",
        "predicted_labels = pd[\"prediction\"].tolist()\n",
        "report = classification_report(true_labels, predicted_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "yH541grYRahr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_nb = best_model_nb.transform(test1)\n",
        "accuracy_nb = evaluator.evaluate(predictions_nb)\n",
        "print(f\"Test Accuracy for Decision Tree: {accuracy_nb:.2f}\")"
      ],
      "metadata": {
        "id": "idvIzt0VRalC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdTWQGyo3PZs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "model_names_1 = ['Decision Tree', 'Random Forest']\n",
        "accuracies_1 = [accuracy,accuracy2]\n",
        "model_names_2 = ['Decision Tree', 'Random Forest']\n",
        "accuracies_2 = [accuracy_dt, accuracy_rf]\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "axes[0].bar(model_names_1, accuracies_1, color=['blue', 'green', 'red', 'purple'])\n",
        "axes[0].set_xlabel('Models')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Model Accuracy Comparison (Set 1)')\n",
        "axes[0].set_ylim(0, 1)\n",
        "axes[1].bar(model_names_2, accuracies_2, color=['blue', 'green', 'red', 'purple'])\n",
        "axes[1].set_xlabel('Models')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Model Accuracy Comparison (Set 2)')\n",
        "axes[1].set_ylim(0, 1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "model_names = ['Decision Tree', 'Random Forest']\n",
        "accuracies = [accuracy, accuracy2]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(model_names, accuracies, color=['blue', 'green', 'red', 'purple'])\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VcTvRlNC4bqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "model_names = ['Decision Tree', 'Random Forest']\n",
        "accuracies = [accuracy_dt, accuracy_rf]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(model_names, accuracies, color=['blue', 'green', 'red', 'purple'])\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l4ampjFlAUic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RK3PpNlFA4IP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}